# AI-friendly Makefile
# All commands return JSON output and include state management

# Default environment variables for test and dev
LOG_LEVEL ?= INFO
USE_MOCK_WEBSOCKET ?= 0
ENVIRONMENT ?= test
REDIS_HOST ?= redis-test
REDIS_PORT ?= 6379
POSTGRES_HOST ?= db-test
POSTGRES_PORT ?= 5432
POSTGRES_USER ?= postgres
POSTGRES_PASSWORD ?= postgres
POSTGRES_DB ?= test_db
ANTHROPIC_API_KEY ?=
PYTHONUNBUFFERED ?= 1

.PHONY: ai-status ai-build ai-restart ai-logs ai-test ai-migrate ai-validate test-setup ai-stop ai-clean ai-down ai-test-websocket ai-test-unit ai-test-integration ai-test-coverage ai-up-test ai-test-log ai-test-stop ai-test-debug ai-test-real ai-test-mock ai-test-redis-mock ai-env-up ai-memory-create ai-memory-destroy ai-coverage-missing ai-test-coverage-missing aio-coverage-report aio-coverage-html aio-coverage-open-html ai-anthropic-ws-test ai-anthropic-http-test frontend-dev frontend-build frontend-preview frontend-lint maintain-rules admin-maintain-rules generate-code-docs admin-generate-code-docs admin-generate-code-story admin-generate-request-lifecycle-story admin-build ai-rebuild-backend-test ai-test-integration-admin ai-list-backend-test ai-dev-build ai-dev-up ai-dev-down ai-dev-logs ai-dev-restart admin-export-knowledge-graph ai-list-admin hot-reload-on hot-reload-off frontend-open frontend-curl frontend-install frontend-restart frontend-build-container frontend-up frontend-down frontend-logs frontend-shell first-run ai-export-knowledge-graph ai-troubleshoot ai-ide-api-build ai-ide-api-up ai-ide-api-down ai-onboarding-health

# Set default coverage threshold, can be overridden: make -f Makefile.ai ai-test-coverage COVERAGE_FAIL_UNDER=40
COVERAGE_FAIL_UNDER ?= 80

# Helper function to format JSON output
define json_output
	printf '{"command":"%s","status":"%s","code":%d,"data":%s}\n' "$(1)" "$(2)" $(3) "$(4)"
endef

# Get current system status
ai-status:
	@services=$$(docker-compose -f docker-compose.test.yml ps --services 2>/dev/null | paste -sd "," - || echo ""); \
	db_status=$$(docker-compose -f docker-compose.test.yml exec -T db-test pg_isready -q 2>/dev/null && echo "ready" || echo "not_ready"); \
	containers=$$(docker-compose -f docker-compose.test.yml ps --services --filter "status=running" 2>/dev/null | wc -l | tr -d ' '); \
	branch=$$(git rev-parse --abbrev-ref HEAD 2>/dev/null | tr -d '\n' || echo "unknown"); \
	commit=$$(git rev-parse HEAD 2>/dev/null | tr -d '\n' || echo "unknown"); \
	printf '{"command":"ai-status","status":"success","code":0,"data":{"services":"%s","database":"%s","containers_running":%s,"git_branch":"%s","last_commit":"%s"}}\n' \
		"$$services" "$$db_status" "$$containers" "$$branch" "$$commit" 2>/dev/null

# Build containers with progress tracking
ai-build:
	@echo '{"status": "starting", "action": "build", "message": "Starting build..."}'
	@docker compose -f docker-compose.test.yml build
	@echo '{"status": "complete", "action": "build", "message": "Build completed successfully"}'

# Restart containers with state verification
ai-restart:
	@(docker-compose -f docker-compose.test.yml down >/dev/null 2>&1; \
	docker-compose -f docker-compose.test.yml up -d >/dev/null 2>&1; \
	sleep 5; \
	healthy_count=$$(docker-compose -f docker-compose.test.yml ps --services --filter "health=healthy" | wc -l | tr -d ' '); \
	total_count=$$(docker-compose -f docker-compose.test.yml ps --services | wc -l | tr -d ' '); \
	if [ $$healthy_count -eq $$total_count ]; then \
		$(call json_output,"ai-restart","success",0,"{\"healthy_containers\":$$healthy_count,\"total_containers\":$$total_count}"); \
	else \
		$(call json_output,"ai-restart","warning",0,"{\"healthy_containers\":$$healthy_count,\"total_containers\":$$total_count}"); \
	fi) 2>/dev/null

# Get logs with structured output
ai-logs:
	@(logs=$$(docker-compose -f docker-compose.test.yml logs --tail=100 2>&1 | sed 's/"/\\"/g' | tr '\n' ' '); \
	$(call json_output,"ai-logs","success",0,"{\"logs\":\"$$logs\"}")) 2>/dev/null

# Run tests with basic status reporting
ai-test:
	@echo '{"status": "starting", "action": "test", "message": "Running tests with args: $(PYTEST_ARGS)"}'
	@make -f Makefile.ai test-setup
	@if echo '$(PYTEST_ARGS)' | grep -q 'mock_service' || [ "$(USE_MOCK_WEBSOCKET)" = "1" ]; then \
	  docker compose -f docker-compose.test.yml exec -e LOG_LEVEL=$(LOG_LEVEL) -e USE_MOCK_WEBSOCKET=1 -T backend-test python -m pytest tests $(PYTEST_ARGS); \
	else \
	  docker compose -f docker-compose.test.yml exec -e LOG_LEVEL=$(LOG_LEVEL) -e USE_MOCK_WEBSOCKET=0 -T backend-test python -m pytest tests $(PYTEST_ARGS); \
	fi
	@echo '{"status": "complete", "action": "test", "message": "Tests complete"}'

# Run migrations with state tracking
ai-migrate:
	@echo '{"status": "starting", "action": "migrate", "message": "Applying Alembic migrations in $(AI_SERVICE) using $(AI_COMPOSE_FILE)..."}'
	@docker-compose -f $(AI_COMPOSE_FILE) exec -T $(AI_SERVICE) alembic upgrade head || echo '{"status": "error", "action": "migrate", "message": "Migration apply failed."}'
	@echo '{"status": "complete", "action": "migrate", "message": "Migration apply complete."}'

# Validate system state
ai-validate:
	@db_ready=$$(docker-compose -f docker-compose.test.yml exec -T db-test pg_isready -q 2>/dev/null && echo "true" || echo "false"); \
	redis_ready=$$(docker-compose -f docker-compose.test.yml exec -T redis-test redis-cli ping >/dev/null 2>&1 && echo "true" || echo "false"); \
	backend_ready=$$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8001/api/v1/health || echo "000"); \
	printf '{"command":"ai-validate","status":"success","code":0,"data":{"database_ready":%s,"redis_ready":%s,"backend_status":"%s"}}\n' \
		"$$db_ready" "$$redis_ready" "$$backend_ready" 2>/dev/null

# Run specific WebSocket tests with detailed output
ai-test-websocket:
	@echo '{"status": "starting", "action": "test-websocket", "message": "Running WebSocket tests..."}'
	@make -f Makefile.ai test-setup
	@(output=$$(docker compose -f docker-compose.test.yml exec -e LOG_LEVEL=$(LOG_LEVEL) -T backend-test pytest tests/test_websocket.py -v $(PYTEST_ARGS) 2>&1); \
	exit_code=$$?; \
	$(call json_output,"ai-test-websocket","$$([ $$exit_code -eq 0 ] && echo 'success' || echo 'error')",$$exit_code,"{\"output\":\"$${output}\"}")) 2>/dev/null

# Run unit tests only (not mock_service or real_service)
ai-test-unit:
	@echo '{"status": "starting", "action": "test-unit", "message": "Running unit tests..."}'
	@make -f Makefile.ai test-setup
	@(output=$$(docker compose -f docker-compose.test.yml exec -e LOG_LEVEL=$(LOG_LEVEL) -e USE_MOCK_WEBSOCKET=1 -T backend-test pytest tests/unit -v -m 'not mock_service and not real_service' $(PYTEST_ARGS) 2>&1); \
	exit_code=$$?; \
	$(call json_output,"ai-test-unit","$$([ $$exit_code -eq 0 ] && echo 'success' || echo 'error')",$$exit_code,"{\"output\":\"$${output}\"}")) 2>/dev/null

# Run integration tests only
ai-test-integration:
	@echo '{"status": "starting", "action": "test-integration", "message": "Running integration tests..."}'
	@make -f Makefile.ai test-setup
	@(output=$$(docker compose -f docker-compose.test.yml exec -e LOG_LEVEL=$(LOG_LEVEL) -T backend-test pytest tests/integration -v $(PYTEST_ARGS) 2>&1); \
	exit_code=$$?; \
	$(call json_output,"ai-test-integration","$$([ $$exit_code -eq 0 ] && echo 'success' || echo 'error')",$$exit_code,"{\"output\":\"$${output}\"}")) 2>/dev/null

# Run tests with coverage reporting
ai-test-coverage:
	@echo '{"status": "starting", "action": "test-coverage", "message": "Running tests with coverage..."}'
	@make -f Makefile.ai test-setup
	@docker compose -f docker-compose.test.yml exec -e LOG_LEVEL=$(LOG_LEVEL) -T backend-test sh -c 'echo "[COVERAGE] Removing old .coverage files..." && rm -f /app/.coverage /app/coverage_data/.coverage && echo "[COVERAGE] Old .coverage files removed."'
	@docker compose -f docker-compose.test.yml exec -e LOG_LEVEL=$(LOG_LEVEL) -T backend-test coverage run -m pytest $(PYTEST_ARGS)
	@docker compose -f docker-compose.test.yml exec -e LOG_LEVEL=$(LOG_LEVEL) -T backend-test sh -c 'echo "[COVERAGE] Before copy: "; ls -lh /app/.coverage || echo "[COVERAGE] /app/.coverage not found"; ls -lh /app/coverage_data; cp /app/.coverage /app/coverage_data/.coverage 2>/dev/null || true; echo "[COVERAGE] After copy: "; ls -lh /app/coverage_data'
	@docker compose -f docker-compose.test.yml exec -e LOG_LEVEL=$(LOG_LEVEL) -T backend-test coverage report --fail-under=$(COVERAGE_FAIL_UNDER) --data-file=/app/coverage_data/.coverage -m

# Target to run tests and log output to a file (Restored Version)
ai-test-log:
	@echo "ðŸš€ Running AI Tests and logging output to test_output.log..." > test_output.log # Overwrite log file initially
	@echo "ðŸ”§ Running pytest with coverage..." >> test_output.log 2>&1
	@docker compose -f docker-compose.test.yml exec -T backend-test pytest \
		--cov=app --cov-report=term-missing --cov-fail-under=$(COVERAGE_FAIL_UNDER) \
		-W ignore::DeprecationWarning -v tests/ >> test_output.log 2>&1 \
	|| (echo "âŒ Tests failed. Output logged to test_output.log." >> test_output.log && exit 1)
	@echo "âœ… Tests completed. Output logged to test_output.log." >> test_output.log
	@cat test_output.log # Display the log output

# Original test setup logic (called by ai-test-log and potentially others)
test-setup:
	@echo "{ \"status\": \"setting up test database\" }"
	@echo "# First terminate all connections to all databases"
	docker compose -f docker-compose.test.yml exec -T db-test psql -U postgres -d postgres -c "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname NOT IN ('postgres') AND pid <> pg_backend_pid();"
	@echo "# Drop test database if it exists"
	docker compose -f docker-compose.test.yml exec -T db-test psql -U postgres -d postgres -c "DROP DATABASE IF EXISTS test_db WITH (FORCE);"
	@echo "# Create test database"
	docker compose -f docker-compose.test.yml exec -T db-test psql -U postgres -d postgres -c "CREATE DATABASE test_db TEMPLATE template0;"
	@echo "# Initialize test database schema using SQLAlchemy"
	@echo 'import asyncio\nfrom app.db.session import init_db\n\nasyncio.run(init_db())' | docker compose -f docker-compose.test.yml exec -T backend-test python
	@echo "{ \"status\": \"test database setup complete\" }"

# Stop containers with JSON output
ai-stop:
	@echo '{"status": "starting", "action": "stop", "message": "Stopping containers..."}'
	@docker compose -f docker-compose.test.yml stop
	@echo '{"status": "complete", "action": "stop", "message": "All containers stopped"}'

# Clean with JSON output
ai-clean:
	@echo '{"status": "starting", "action": "clean", "message": "Cleaning project..."}'
	@docker compose -f docker-compose.test.yml down -v
	@echo '{"status": "complete", "action": "clean", "message": "Clean completed"}'

# Down containers and remove volumes with JSON output
ai-down:
	@echo '{"status": "starting", "action": "down", "message": "Stopping and removing containers..."}'
	@docker compose -f docker-compose.test.yml down -v
	@echo '{"status": "complete", "action": "down", "message": "All containers stopped and volumes removed"}'

# Start test environment with JSON output
ai-up-test:
	@echo '{"status": "starting", "action": "up-test", "message": "Starting test environment..."}'
	@docker compose -f docker-compose.test.yml up -d
	@echo "Waiting for services to be ready..."
	@sleep 5
	@(healthy_count=$$(docker-compose -f docker-compose.test.yml ps --services --filter "health=healthy" | wc -l | tr -d ' '); \
	total_count=$$(docker-compose -f docker-compose.test.yml ps --services | wc -l | tr -d ' '); \
	if [ $$healthy_count -eq $$total_count ]; then \
		$(call json_output,"ai-up-test","success",0,"{\"healthy_containers\":$$healthy_count,\"total_containers\":$$total_count,\"message\":\"Test environment ready\"}"); \
	else \
		$(call json_output,"ai-up-test","warning",1,"{\"healthy_containers\":$$healthy_count,\"total_containers\":$$total_count,\"message\":\"Some containers not healthy\"}"); \
	fi) 2>/dev/null

# Default help command
help:
	@echo "Available AI-friendly commands:"
	@echo "  make -f Makefile.ai ai-status     - Get current system status in JSON format"
	@echo "  make -f Makefile.ai ai-build      - Build containers with progress tracking"
	@echo "  make -f Makefile.ai ai-restart    - Restart containers with health checks"
	@echo "  make -f Makefile.ai ai-logs       - Get structured log output"
	@echo "  make -f Makefile.ai ai-test       - Run all tests with basic status reporting"
	@echo "  make -f Makefile.ai ai-test-websocket - Run WebSocket tests specifically"
	@echo "  make -f Makefile.ai ai-test-unit  - Run unit tests only"
	@echo "  make -f Makefile.ai ai-test-integration - Run integration tests only"
	@echo "  make -f Makefile.ai ai-migrate    - Run migrations with state tracking"
	@echo "  make -f Makefile.ai ai-validate   - Validate system state"
	@echo "  make -f Makefile.ai test-setup    - Set up test environment"
	@echo "  make -f Makefile.ai ai-stop       - Stop containers"
	@echo "  make -f Makefile.ai ai-clean      - Clean project"
	@echo "  make -f Makefile.ai ai-down       - Stop and remove containers with volumes"
	@echo "  make -f Makefile.ai ai-test-coverage - Run tests with coverage reporting"
	@echo "  make -f Makefile.ai ai-up-test    - Start test environment containers"
	@echo "  make -f Makefile.ai ai-test-log    - Run tests and log output"
	@echo "  make -f Makefile.ai ai-memory-create  - Create claude-memory Docker volume"
	@echo "  make -f Makefile.ai ai-memory-destroy - Remove claude-memory Docker volume"
	@echo "  make -f Makefile.ai ai-coverage-missing - Show files and lines missing coverage (for automation)"

# Stop test containers with JSON output
ai-test-stop:
	@echo "{ \"status\": \"stopping test containers\" }"
	@docker compose -f docker-compose.test.yml down --remove-orphans
	@echo "{ \"status\": \"test containers stopped\" }"

# Run tests in debug mode
ai-test-debug:
	@echo "Running tests in debug mode with detailed output..."
	docker compose -f docker-compose.test.yml exec -T backend-test python -m pytest \
		--verbose \
		--capture=no \
		--log-cli-level=DEBUG \
		--log-cli-format="%(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s)" \
		--showlocals \
		--tb=long \
		-vv \
		$(PYTEST_ARGS)

# Run tests with the real_service marker
ai-test-real:
	@echo '{"status": "starting", "action": "test-real", "message": "Running real_service tests..."}'
	@make -f Makefile.ai test-setup
	@(output=$$(docker compose -f docker-compose.test.yml exec -T backend-test pytest -m real_service tests -v $(PYTEST_ARGS) 2>&1); \
	exit_code=$$?; \
	$(call json_output,"ai-test-real","$$([ $$exit_code -eq 0 ] && echo 'success' || echo 'error')",$$exit_code,"{\"output\":\"$${output}\"}")) 2>/dev/null

# Run all tests with the mock_service marker (anywhere in tests/)
ai-test-mock:
	@echo '{"status": "starting", "action": "test-mock", "message": "Running mock_service tests..."}'
	@make -f Makefile.ai test-setup
	# MOCK_WS_CONNECT_ERROR is now set per-test via pytest marker/fixture, not globally
	@(output=$$(docker compose -f docker-compose.test.yml exec -e USE_MOCK_WEBSOCKET=1 -T backend-test pytest -m mock_service tests -v $(PYTEST_ARGS) 2>&1); \
	exit_code=$$?; \
	$(call json_output,"ai-test-mock","$$([ $$exit_code -eq 0 ] && echo 'success' || echo 'error')",$$exit_code,"{\"output\":\"$${output}\"}")) 2>/dev/null

# Run Redis mock tests
ai-test-redis-mock:
	@echo '{"status": "starting", "action": "test-redis-mock", "message": "Running Redis mock tests..."}'
	@(output=$$(PYTHONPATH=backend pytest -v backend/tests/test_redis_mock.py -m unit 2>&1); \
	exit_code=$$?; \
	$(call json_output,"ai-test-redis-mock","$$([ $$exit_code -eq 0 ] && echo 'success' || echo 'error')",$$exit_code,"{\"output\":\"$${output}\"}")) 2>/dev/null

# Start test environment and ensure services are ready
ai-env-up:
	@echo '{"status": "starting", "action": "env-up", "message": "Starting test environment..."}'
	@docker compose -f docker-compose.test.yml up -d
	@echo "Waiting for services to be ready..."
	@sleep 5
	@(healthy_count=$$(docker-compose -f docker-compose.test.yml ps --services --filter "health=healthy" | wc -l | tr -d ' '); \
	total_count=$$(docker-compose -f docker-compose.test.yml ps --services | wc -l | tr -d ' '); \
	if [ $$healthy_count -eq $$total_count ]; then \
		$(call json_output,"ai-env-up","success",0,"{\"healthy_containers\":$$healthy_count,\"total_containers\":$$total_count,\"message\":\"Test environment ready\"}"); \
	else \
		$(call json_output,"ai-env-up","warning",1,"{\"healthy_containers\":$$healthy_count,\"total_containers\":$$total_count,\"message\":\"Some containers not healthy\"}"); \
	fi) 2>/dev/null
	@make -f Makefile.ai test-setup

# Create claude-memory Docker volume
ai-memory-create:
	@volume_exists=$$(docker volume ls -q -f name=claude-memory | grep -q claude-memory && echo "true" || echo "false"); \
	if [ "$$volume_exists" = "true" ]; then \
		echo '{"command":"ai-memory-create","status":"skipped","code":0,"data":{"message":"Volume claude-memory already exists"}}'; \
	else \
		docker volume create claude-memory > /dev/null 2>&1; \
		echo '{"command":"ai-memory-create","status":"success","code":0,"data":{"message":"Created claude-memory volume"}}'; \
	fi

# Remove claude-memory Docker volume
ai-memory-destroy:
	@volume_exists=$$(docker volume ls -q -f name=claude-memory | grep -q claude-memory && echo "true" || echo "false"); \
	if [ "$$volume_exists" = "true" ]; then \
		docker volume rm claude-memory > /dev/null 2>&1; \
		echo '{"command":"ai-memory-destroy","status":"success","code":0,"data":{"message":"Removed claude-memory volume"}}'; \
	else \
		echo '{"command":"ai-memory-destroy","status":"skipped","code":0,"data":{"message":"Volume claude-memory does not exist"}}'; \
	fi

# Show files and lines missing coverage (for automation)
ai-coverage-missing:
	@docker compose -f docker-compose.test.yml exec -T backend-test env COVERAGE_FILE=/app/coverage_data/.coverage coverage report -m | grep -v '100%' || true

# Run tests with coverage and immediately show missing lines
ai-test-coverage-missing:
	@echo '{"status": "starting", "action": "test-coverage-missing", "message": "Running tests with coverage and showing missing lines..."}'
	@make -f Makefile.ai test-setup
	@docker compose -f docker-compose.test.yml exec -T backend-test sh -c 'echo "[COVERAGE] Removing old .coverage files..." && rm -f /app/.coverage /app/coverage_data/.coverage && echo "[COVERAGE] Old .coverage files removed."'
	@docker compose -f docker-compose.test.yml exec -T backend-test coverage run -m pytest $(PYTEST_ARGS)
	@docker compose -f docker-compose.test.yml exec -T backend-test sh -c 'echo "[COVERAGE] Before copy: "; ls -lh /app/.coverage || echo "[COVERAGE] /app/.coverage not found"; ls -lh /app/coverage_data; cp /app/.coverage /app/coverage_data/.coverage 2>/dev/null || true; echo "[COVERAGE] After copy: "; ls -lh /app/coverage_data'
	@docker compose -f docker-compose.test.yml exec -T backend-test coverage report --data-file=/app/coverage_data/.coverage -m | grep -v '100%' || true

# Generate terminal coverage report from persisted file
aio-coverage-report:
	@coverage report --data-file=coverage_data/.coverage -m

# Generate HTML coverage report from persisted file
aio-coverage-html:
	@coverage html --data-file=coverage_data/.coverage

# Open HTML coverage report in default browser (macOS)
aio-coverage-open-html:
	@open htmlcov/index.html

# Post a test message to Anthropic's API
ai-anthropic-ws-test:
	docker compose -f docker-compose.test.yml exec backend-test python /app/scripts/anthropic_ws_direct.py

ai-anthropic-http-test:
	docker compose -f docker-compose.test.yml exec backend-test python /app/scripts/anthropic_http_post.py

# Run only mock_service tests in integration/websocket (always set USE_MOCK_WEBSOCKET=1)
ai-test-websocket-mock:
	@echo '{"status": "starting", "action": "test-websocket-mock", "message": "Running WebSocket mock_service tests..."}'
	@make -f Makefile.ai test-setup
	@(output=$$(docker compose -f docker-compose.test.yml exec -e USE_MOCK_WEBSOCKET=1 -T backend-test pytest tests/integration/websocket -m mock_service -v $(PYTEST_ARGS) 2>&1); \
	exit_code=$$?; \
	$(call json_output,"ai-test-websocket-mock","$$([ $$exit_code -eq 0 ] && echo 'success' || echo 'error')",$$exit_code,"{\"output\":\"$${output}\"}")) 2>/dev/null

# Marker-specific test targets for new marker scheme
#
# Usage examples:
#   make -f Makefile.ai ai-test-real-websocket   # Run only tests marked with real_websocket
#   make -f Makefile.ai ai-test-real-redis       # Run only tests marked with real_redis
#   make -f Makefile.ai ai-test-real-anthropic   # Run only tests marked with real_anthropic
#   make -f Makefile.ai ai-test-websocket-mock   # Run only tests marked with mock_service (WebSocket)
#
# These targets ensure the correct environment variables are set for each test type.

ai-test-real-websocket:
	@echo '{"status": "starting", "action": "test-real-websocket", "message": "Running real_websocket tests..."}'
	@make -f Makefile.ai test-setup
	@(output=$$(docker compose -f docker-compose.test.yml exec -T backend-test pytest -m real_websocket tests -v $(PYTEST_ARGS) 2>&1); \
	exit_code=$$?; \
	$(call json_output,"ai-test-real-websocket","$$([ $$exit_code -eq 0 ] && echo 'success' || echo 'error')",$$exit_code,"{\"output\":\"$${output}\"}")) 2>/dev/null

ai-test-real-redis:
	@echo '{"status": "starting", "action": "test-real-redis", "message": "Running real_redis tests..."}'
	@make -f Makefile.ai test-setup
	@(output=$$(docker compose -f docker-compose.test.yml exec -e REDIS_HOST=redis-test -T backend-test pytest -m real_redis tests -v $(PYTEST_ARGS) 2>&1); \
	exit_code=$$?; \
	$(call json_output,"ai-test-real-redis","$$([ $$exit_code -eq 0 ] && echo 'success' || echo 'error')",$$exit_code,"{\"output\":\"$${output}\"}")) 2>/dev/null

ai-test-real-anthropic:
	@echo '{"status": "starting", "action": "test-real-anthropic", "message": "Running real_anthropic tests..."}'
	@make -f Makefile.ai test-setup
	@(output=$$(docker compose -f docker-compose.test.yml exec -e ANTHROPIC_API_KEY=$$ANTHROPIC_API_KEY -T backend-test pytest -m real_anthropic tests -v $(PYTEST_ARGS) 2>&1); \
	exit_code=$$?; \
	$(call json_output,"ai-test-real-anthropic","$$([ $$exit_code -eq 0 ] && echo 'success' || echo 'error')",$$exit_code,"{\"output\":\"$${output}\"}")) 2>/dev/null

# Marker-specific test targets with fail-fast, verbose, and short traceback output
#
# Usage examples:
#   make -f Makefile.ai ai-test-websocket-mock-xv   # Mock WebSocket tests, fail-fast, verbose, short traceback
#   make -f Makefile.ai ai-test-real-websocket-xv  # Real WebSocket tests, fail-fast, verbose, short traceback
#   make -f Makefile.ai ai-test-real-redis-xv      # Real Redis tests, fail-fast, verbose, short traceback
#   make -f Makefile.ai ai-test-real-anthropic-xv  # Real Anthropic tests, fail-fast, verbose, short traceback
#
# These targets are for quick debugging of the first failing test in each marker group.

ai-test-websocket-mock-xv:
	@docker compose -f docker-compose.test.yml exec -e USE_MOCK_WEBSOCKET=1 -T backend-test pytest tests/integration/websocket -m mock_service -x -v --tb=short $(PYTEST_ARGS)

ai-test-real-websocket-xv:
	@$(MAKE) -f Makefile.ai ai-test-real-websocket PYTEST_ARGS='-x -v --tb=short' $(MAKECMDGOALS)

ai-test-real-redis-xv:
	@$(MAKE) -f Makefile.ai ai-test-real-redis PYTEST_ARGS='-x -v --tb=short' $(MAKECMDGOALS)

ai-test-real-anthropic-xv:
	@$(MAKE) -f Makefile.ai ai-test-real-anthropic PYTEST_ARGS='-x -v --tb=short' $(MAKECMDGOALS)

# Marker-specific test target for mock WebSocket with fail-fast, verbose, short traceback, and output logging
# Now sets PYTHONUNBUFFERED=1 to ensure all output is flushed to the log file.
ai-test-websocket-mock-xv-log:
	@echo 'Running mock WebSocket tests with -x -v --tb=short and logging output to test_output.log...'
	@docker compose -f docker-compose.test.yml exec -e USE_MOCK_WEBSOCKET=1 -T backend-test sh -c 'PYTHONUNBUFFERED=1 pytest -m mock_service -x -v --tb=short --maxfail=1' > test_output.log 2>&1
	@cat test_output.log

# Run Redis mock tests with fail-fast, verbose, and short traceback output
ai-test-redis-mock-xv:
	@echo 'Running Redis mock tests with -x -v --tb=short...'
	@PYTHONPATH=backend pytest -x -v --tb=short backend/tests/utils/test_mock_redis.py backend/tests/unit/test_mock_redis.py

# Add new target ai-test-redis-mock-sync
ai-test-redis-mock-sync:
	@echo 'Running sync Redis mock tests...'
	PYTHONPATH=backend pytest backend/tests/utils/test_sync_mock_redis.py -v

frontend-dev:
	cd frontend && npm run dev

frontend-build:
	cd frontend && npm run build

frontend-preview:
	cd frontend && npm run preview

frontend-lint:
	cd frontend && npm run lint

frontend-open:
	@open http://localhost:3000

frontend-curl:
	@curl -i http://localhost:3000

maintain-rules:
	python maintain_rules.py

admin-maintain-rules:
	docker compose -f docker-compose.admin.yml run --rm admin python maintain_rules.py

# generate-code-docs:
# 	python generate_code_docs.py
# (Prefer running the admin-generate-code-docs target via the admin container for consistency)

admin-generate-code-docs:
	docker compose -f docker-compose.admin.yml run --rm admin python scripts/generate_code_docs.py

# Requires Ollama to be running locally and accessible from the container
admin-generate-code-story:
	docker compose -f docker-compose.admin.yml run --rm admin python scripts/generate_code_story.py

# Requires Ollama to be running locally and accessible from the container
admin-generate-request-lifecycle-story:
	docker compose -f docker-compose.admin.yml run --rm admin python scripts/generate_request_lifecycle_story.py

# Add new target admin-build
admin-build:
	docker compose -f docker-compose.admin.yml build admin

# AI-friendly Alembic migration workflow
# Usage:
#   make -f Makefile.ai ai-migrate-init [COMPOSE_FILE=docker-compose.test.yml] [SERVICE=backend-test]
#   make -f Makefile.ai ai-migrate-create name=desc [COMPOSE_FILE=docker-compose.test.yml] [SERVICE=backend-test]
#   make -f Makefile.ai ai-migrate [COMPOSE_FILE=docker-compose.test.yml] [SERVICE=backend-test]
#   make -f Makefile.ai ai-migrate-rollback [COMPOSE_FILE=docker-compose.test.yml] [SERVICE=backend-test]

AI_COMPOSE_FILE ?= docker-compose.test.yml
AI_SERVICE ?= backend-test

ai-migrate-init:
	@echo '{"status": "starting", "action": "migrate-init", "message": "Initializing Alembic in $(AI_SERVICE) using $(AI_COMPOSE_FILE)..."}'
	@docker-compose -f $(AI_COMPOSE_FILE) exec -T $(AI_SERVICE) alembic init alembic || echo '{"status": "error", "action": "migrate-init", "message": "Alembic init failed."}'
	@echo '{"status": "complete", "action": "migrate-init", "message": "Alembic initialization complete."}'

ai-migrate-create:
	@echo '{"status": "starting", "action": "migrate-create", "message": "Creating Alembic migration in $(AI_SERVICE) using $(AI_COMPOSE_FILE)..."}'
	@docker-compose -f $(AI_COMPOSE_FILE) exec -T $(AI_SERVICE) python3 /app/scripts/create_migration.py "$(name)" || echo '{"status": "error", "action": "migrate-create", "message": "Migration creation failed."}'
	@echo '{"status": "complete", "action": "migrate-create", "message": "Migration creation complete."}'

ai-migrate:
	@echo '{"status": "starting", "action": "migrate", "message": "Applying Alembic migrations in $(AI_SERVICE) using $(AI_COMPOSE_FILE)..."}'
	@docker-compose -f $(AI_COMPOSE_FILE) exec -T $(AI_SERVICE) alembic upgrade head || echo '{"status": "error", "action": "migrate", "message": "Migration apply failed."}'
	@echo '{"status": "complete", "action": "migrate", "message": "Migration apply complete."}'

ai-migrate-rollback:
	@echo '{"status": "starting", "action": "migrate-rollback", "message": "Rolling back Alembic migration in $(AI_SERVICE) using $(AI_COMPOSE_FILE)..."}'
	@docker-compose -f $(AI_COMPOSE_FILE) exec -T $(AI_SERVICE) alembic downgrade -1 || echo '{"status": "error", "action": "migrate-rollback", "message": "Migration rollback failed."}'
	@echo '{"status": "complete", "action": "migrate-rollback", "message": "Migration rollback complete."}'

ai-rebuild-backend-test:
	@echo '{"status": "starting", "action": "rebuild-backend-test", "message": "Forcing clean rebuild of backend-test container..."}'
	@docker-compose -f docker-compose.test.yml build --no-cache backend-test
	@echo '{"status": "complete", "action": "rebuild-backend-test", "message": "backend-test container rebuilt with no cache."}'

# Run only the admin integration tests (AI-friendly, non-interactive)
ai-test-integration-admin:
	docker-compose -f docker-compose.test.yml run --rm backend-test pytest -vv tests/integration/test_admin.py

# List files in a directory inside the backend-test container (non-interactive)
ai-list-backend-test:
	docker-compose -f docker-compose.test.yml run --rm backend-test ls -lR /app

# List files in a directory inside the admin container (non-interactive)
ai-list-admin:
	docker compose -f docker-compose.admin.yml run --rm admin ls -lR /admin

# AI-friendly dev environment targets
ai-dev-build:
	@echo '{"status": "starting", "action": "dev-build", "message": "Building dev containers..."}'
	@docker-compose -f docker-compose.dev.yml build
	@echo '{"status": "complete", "action": "dev-build", "message": "Dev build complete."}'

ai-dev-up:
	@echo '{"status": "starting", "action": "dev-up", "message": "Starting dev containers..."}'
	@docker-compose -f docker-compose.dev.yml up -d
	@echo '{"status": "complete", "action": "dev-up", "message": "Dev containers started."}'

ai-dev-down:
	@echo '{"status": "starting", "action": "dev-down", "message": "Stopping and removing dev containers..."}'
	@docker-compose -f docker-compose.dev.yml down -v
	@echo '{"status": "complete", "action": "dev-down", "message": "Dev containers stopped and volumes removed."}'

ai-dev-logs:
	@docker-compose -f docker-compose.dev.yml logs --tail=100 | sed 's/"/\\"/g' | tr '\n' ' '

ai-dev-restart:
	@docker-compose -f docker-compose.dev.yml down
	@docker-compose -f docker-compose.dev.yml up -d
	@echo '{"status": "complete", "action": "dev-restart", "message": "Dev containers restarted."}'

# Export the knowledge graph (Mermaid) to docs/cursor_knowledge_graph.md
admin-export-knowledge-graph:
	docker compose -f docker-compose.admin.yml run --rm admin python scripts/export_knowledge_graph.py

clean-logs:
	rm -f *.log */*.log */*/*.log

# Toggle hot reload for backend-test in docker-compose.test.yml
# Usage: make -f Makefile.ai hot-reload-on | hot-reload-off

hot-reload-on:
	@sed -i '' 's/^# \(- \.\/backend:\/app\)/\1/' docker-compose.test.yml
	@echo 'Hot reload ENABLED: ./backend is now mounted into backend-test container.'

hot-reload-off:
	@sed -i '' 's/^\(- \.\/backend:\/app\)/# \1/' docker-compose.test.yml
	@echo 'Hot reload DISABLED: ./backend is no longer mounted into backend-test container.'

frontend-install:
	docker-compose -f docker-compose.dev.yml exec frontend npm install

frontend-restart:
	docker-compose -f docker-compose.dev.yml restart frontend

frontend-build-container:
	docker-compose -f docker-compose.dev.yml build frontend

frontend-up:
	docker-compose -f docker-compose.dev.yml up -d frontend

frontend-down:
	docker-compose -f docker-compose.dev.yml down

frontend-logs:
	docker-compose -f docker-compose.dev.yml logs -f frontend

frontend-shell:
	docker-compose -f docker-compose.dev.yml exec frontend sh

.PHONY: first-run
first-run:
	bash ./first_run.sh

.PHONY: ai-export-knowledge-graph
ai-export-knowledge-graph:
	python admin/scripts/export_knowledge_graph.py

.PHONY: ai-troubleshoot
ai-troubleshoot:
	bash ./ai_troubleshoot.sh

.PHONY: ai-ide-api-build ai-ide-api-up ai-ide-api-down
ai-ide-api-build:
	docker build -t ai-ide-api ./ai-ide-api

ai-ide-api-up:
	docker run --rm -d -p 8080:8080 --name ai-ide-api \
	  -v $$(pwd)/onboarding.yaml:/app/onboarding.yaml \
	  -v $$(pwd)/.ai-ide-config.json:/app/.ai-ide-config.json \
	  -v $$(pwd)/docs:/app/docs \
	  -v $$(pwd)/.cursor/rules:/app/.cursor/rules \
	  ai-ide-api

ai-ide-api-down:
	docker stop ai-ide-api || true

.PHONY: ai-onboarding-health
ai-onboarding-health:
	bash ./ai_onboarding_checklist.sh
	python lint_rules.py
	@echo "\nReminder: If you made major onboarding or rules changes, update ONBOARDING_CHANGELOG.md and bump the version in .ai-ide-config.json!"